{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "from crawler import crawler, pattern_filter\n",
    "\n",
    "\n",
    "url_queue = asyncio.Queue()\n",
    "response_queue = asyncio.Queue()\n",
    "stop_crawling = asyncio.Event()\n",
    "await url_queue.put(\"https://caseyhandmer.wordpress.com/\")\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "addable_urls = []\n",
    "async with httpx.AsyncClient() as client:\n",
    "    addable_urls = await crawler(\n",
    "        url_queue,\n",
    "        url_filter={\"filter_func\": pattern_filter, \"kwargs\": {\"regex_patterns\": [\"https://\"]}},\n",
    "        client=client,\n",
    "        response_queue=response_queue,\n",
    "        pause=stop_crawling,\n",
    "        end=stop_crawling,\n",
    "        max_iter=1,\n",
    "    )\n",
    "    print(\"addable urls:\", addable_urls)\n",
    "    stop_crawling.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from joblib import load\n",
    "\n",
    "addable_urls = load(\"cleanable_urls.joblib\")\n",
    "\n",
    "def clean_urls(urls):\n",
    "    cleaned_urls = set()  # Use a set to ensure no duplicates\n",
    "\n",
    "    # Remove fragments and trailing slashes\n",
    "    for url in urls:\n",
    "        parsed_url = urlparse(url)\n",
    "\n",
    "        cleaned_url = parsed_url._replace(fragment=\"\")\n",
    "\n",
    "        final_url = urlunparse(cleaned_url).rstrip(\"/\")\n",
    "\n",
    "        cleaned_urls.add(final_url)\n",
    "\n",
    "    return list(cleaned_urls) \n",
    "\n",
    "print(len(addable_urls))\n",
    "cleaned_urls = clean_urls(addable_urls)\n",
    "print(len(cleaned_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the processor\n",
    "\n",
    "- `multi-qa-MiniLM-L6-cos-v1` is one of the best models for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from joblib import load\n",
    "import qdrant_client\n",
    "\n",
    "soup = load(\"test_data/html_page.joblib\")\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "client = qdrant_client.QdrantClient(\":memory:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "def extract_visible_text(soup):\n",
    "    # Remove elements that do not contain user-visible text\n",
    "    for element in soup(['script', 'style', 'meta', 'header', 'footer', 'nav', 'noscript']):\n",
    "        element.decompose()  # Removes the element from the soup\n",
    "\n",
    "    # Extract the raw text\n",
    "    raw_text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Clean up the extracted text\n",
    "    visible_text = re.sub(r'\\s+', ' ', raw_text).strip()  # Replaces multiple spaces/newlines with a single space\n",
    "    \n",
    "    return visible_text\n",
    "\n",
    "def process_html_to_vectors(\n",
    "    soup: BeautifulSoup,\n",
    "    model: sentence_transformers.SentenceTransformer,\n",
    "    db_client: QdrantClient,\n",
    "    max_length: int = 450,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes a BeautifulSoup object into a list of sentences and turns each of them\n",
    "    into a vector using the sentence_transformers model. Puts the vectors into a Qdrant\n",
    "    collection.\n",
    "\n",
    "    This is test code for now.\n",
    "    \"\"\"\n",
    "    # Extract visible text from the soup\n",
    "    visible_text = extract_visible_text(soup)\n",
    "    \n",
    "    # Get splits of 450 words\n",
    "    split_text = visible_text.split(' ')\n",
    "    splits = list(range(0, len(split_text), max_length))\n",
    "    splits.append(len(split_text))\n",
    "\n",
    "    # Create the sequences\n",
    "    sequences = [' '.join(split_text[i:j]) for i, j in zip(splits[:-1], splits[1:])]\n",
    "\n",
    "    # Turns the sequences into float16 vectors\n",
    "    vectors = model.encode(sequences, convert_to_numpy=True)\n",
    "    vectors = vectors.astype(np.float32)\n",
    "\n",
    "    # Create the collection\n",
    "    if not db_client.collection_exists(\"my_collection\"):\n",
    "        db_client.create_collection(\n",
    "        collection_name=\"my_collection\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # Add the vectors to the collection\n",
    "    db_client.upsert(\n",
    "        collection_name=\"my_collection\",\n",
    "        points=[\n",
    "            PointStruct(id=idx, vector=vector.tolist(), payload={\"text\": sequences[idx]})\n",
    "            for idx, vector in enumerate(vectors)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "process_html_to_vectors(soup, model, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.encode(\"what does casey handmer do?\", convert_to_numpy=True)\n",
    "hits = client.search(\n",
    "   collection_name=\"my_collection\",\n",
    "   query_vector=vec,\n",
    "   limit=5  # Return 5 closest points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl CJ Handmer's blog for 300 links\n",
    "\n",
    "- Do this and store all the data in the postgres and vector databases.\n",
    "- Use this data to run tests using the search module.\n",
    "- Use this data to serve a local site that provides a simple search interface that OpenEngine will have.\n",
    "- Build the interface using Svelte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the inputs for `gather.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code for the process\n",
    "import setup_postgres\n",
    "import psycopg2\n",
    "import qdrant_client\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up the postgres connection\n",
    "temp_dir, port = setup_postgres.start_ephemeral_postgres()\n",
    "\n",
    "client = psycopg2.connect(\n",
    "    dbname='postgres',\n",
    "    user='postgres',\n",
    "    host=\"localhost\",\n",
    "    port=port\n",
    ")\n",
    "print(\"connected to postgres\")\n",
    "\n",
    "cursor = client.cursor()\n",
    "\n",
    "# Create the table\n",
    "table_sql = \"\"\"CREATE TABLE resources ( \n",
    "    id SERIAL PRIMARY KEY,\n",
    "    url VARCHAR(2048) NOT NULL,\n",
    "    firstVisited TIMESTAMP NOT NULL,\n",
    "    lastVisited TIMESTAMP NOT NULL,\n",
    "    allVisits INT DEFAULT 1,\n",
    "    externalLinks TEXT[]\n",
    ");\"\"\"\n",
    "\n",
    "cursor.execute(table_sql)\n",
    "\n",
    "# Add handmer's website\n",
    "insert_handmer = \"\"\"INSERT INTO resources (url, firstVisited, lastVisited, allVisits, externalLinks) VALUES (%s, %s, %s, %s, %s)\"\"\"\n",
    "handmer_data = (\"https://caseyhandmer.wordpress.com\", datetime.now(), datetime.now(), 0, [])\n",
    "\n",
    "cursor.execute(insert_handmer, handmer_data)\n",
    "\n",
    "# Set up the qdrant connection\n",
    "vector_client = qdrant_client.QdrantClient(\":memory:\")\n",
    "vector_client.create_collection(\n",
    "    collection_name=\"embeddings\",\n",
    "    vectors_config=qdrant_client.models.VectorParams(size=384, distance=qdrant_client.models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# Set up the revisit delta\n",
    "from datetime import timedelta\n",
    "revisit_delta = timedelta(microseconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the postgres and qdrant servers\n",
    "setup_postgres.stop_ephemeral_postgres(temp_dir)\n",
    "del vector_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gather\n",
    "\n",
    "# Set up the max iterations\n",
    "max_iter = 75\n",
    "\n",
    "await gather.gather(vector_client, client, model, revisit_delta, max_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check vector database and normal database for urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the vector and metadata were stored correctly\n",
    "points = vector_client.scroll(\n",
    "    collection_name=\"embeddings\",\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    "    limit=10_000,\n",
    ")[0]\n",
    "\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get resources from the postgres server\n",
    "sql = \"select * from resources;\"\n",
    "cursor.execute(sql)\n",
    "all_meta = cursor.fetchall()\n",
    "\n",
    "print(len(all_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the metadata and vector data for use in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(all_meta, \"test_data/search_engine/postgres_meta.joblib\")\n",
    "dump(points, \"test_data/search_engine/qdrant_vectors.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
