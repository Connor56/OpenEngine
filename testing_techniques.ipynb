{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "from crawler import crawler, pattern_filter\n",
    "\n",
    "\n",
    "url_queue = asyncio.Queue()\n",
    "response_queue = asyncio.Queue()\n",
    "stop_crawling = asyncio.Event()\n",
    "await url_queue.put(\"https://caseyhandmer.wordpress.com/\")\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "addable_urls = []\n",
    "async with httpx.AsyncClient() as client:\n",
    "    addable_urls = await crawler(\n",
    "        url_queue,\n",
    "        url_filter={\"filter_func\": pattern_filter, \"kwargs\": {\"regex_patterns\": [\"https://\"]}},\n",
    "        client=client,\n",
    "        response_queue=response_queue,\n",
    "        pause=stop_crawling,\n",
    "        end=stop_crawling,\n",
    "        max_iter=1,\n",
    "    )\n",
    "    print(\"addable urls:\", addable_urls)\n",
    "    stop_crawling.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from joblib import load\n",
    "\n",
    "addable_urls = load(\"cleanable_urls.joblib\")\n",
    "\n",
    "def clean_urls(urls):\n",
    "    cleaned_urls = set()  # Use a set to ensure no duplicates\n",
    "\n",
    "    # Remove fragments and trailing slashes\n",
    "    for url in urls:\n",
    "        parsed_url = urlparse(url)\n",
    "\n",
    "        cleaned_url = parsed_url._replace(fragment=\"\")\n",
    "\n",
    "        final_url = urlunparse(cleaned_url).rstrip(\"/\")\n",
    "\n",
    "        cleaned_urls.add(final_url)\n",
    "\n",
    "    return list(cleaned_urls) \n",
    "\n",
    "print(len(addable_urls))\n",
    "cleaned_urls = clean_urls(addable_urls)\n",
    "print(len(cleaned_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the processor\n",
    "\n",
    "- `multi-qa-MiniLM-L6-cos-v1` is one of the best models for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from joblib import load\n",
    "import qdrant_client\n",
    "\n",
    "soup = load(\"test_data/html_page.joblib\")\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "client = qdrant_client.QdrantClient(\":memory:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "def extract_visible_text(soup):\n",
    "    # Remove elements that do not contain user-visible text\n",
    "    for element in soup(['script', 'style', 'meta', 'header', 'footer', 'nav', 'noscript']):\n",
    "        element.decompose()  # Removes the element from the soup\n",
    "\n",
    "    # Extract the raw text\n",
    "    raw_text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Clean up the extracted text\n",
    "    visible_text = re.sub(r'\\s+', ' ', raw_text).strip()  # Replaces multiple spaces/newlines with a single space\n",
    "    \n",
    "    return visible_text\n",
    "\n",
    "def process_html_to_vectors(\n",
    "    soup: BeautifulSoup,\n",
    "    model: sentence_transformers.SentenceTransformer,\n",
    "    db_client: QdrantClient,\n",
    "    max_length: int = 450,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes a BeautifulSoup object into a list of sentences and turns each of them\n",
    "    into a vector using the sentence_transformers model. Puts the vectors into a Qdrant\n",
    "    collection.\n",
    "\n",
    "    This is test code for now.\n",
    "    \"\"\"\n",
    "    # Extract visible text from the soup\n",
    "    visible_text = extract_visible_text(soup)\n",
    "    \n",
    "    # Get splits of 450 words\n",
    "    split_text = visible_text.split(' ')\n",
    "    splits = list(range(0, len(split_text), max_length))\n",
    "    splits.append(len(split_text))\n",
    "\n",
    "    # Create the sequences\n",
    "    sequences = [' '.join(split_text[i:j]) for i, j in zip(splits[:-1], splits[1:])]\n",
    "\n",
    "    # Turns the sequences into float16 vectors\n",
    "    vectors = model.encode(sequences, convert_to_numpy=True)\n",
    "    vectors = vectors.astype(np.float32)\n",
    "\n",
    "    # Create the collection\n",
    "    if not db_client.collection_exists(\"my_collection\"):\n",
    "        db_client.create_collection(\n",
    "        collection_name=\"my_collection\",\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # Add the vectors to the collection\n",
    "    db_client.upsert(\n",
    "        collection_name=\"my_collection\",\n",
    "        points=[\n",
    "            PointStruct(id=idx, vector=vector.tolist(), payload={\"text\": sequences[idx]})\n",
    "            for idx, vector in enumerate(vectors)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "process_html_to_vectors(soup, model, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.encode(\"what does casey handmer do?\", convert_to_numpy=True)\n",
    "hits = client.search(\n",
    "   collection_name=\"my_collection\",\n",
    "   query_vector=vec,\n",
    "   limit=5  # Return 5 closest points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl CJ Handmer's blog for 300 links\n",
    "\n",
    "- Do this and store all the data in the postgres and vector databases.\n",
    "- Use this data to run tests using the search module.\n",
    "- Use this data to serve a local site that provides a simple search interface that OpenEngine will have.\n",
    "- Build the interface using Svelte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
